import pandas as pd
import config
import os
from settings import RESULT_DIR
from debt_utils import map_ground_truth_label

def evaluate_td_per_line(ground_truth, normalized_preds):
    """Compute per-line metrics for technical debt detection."""
    per_line = []
    TP = FP = TN = FN = 0
    total_correct = 0

    for idx, (gt_entry, pred) in enumerate(zip(ground_truth, normalized_preds), start=1):
        true_label = map_ground_truth_label(gt_entry)
        pred_positive = pred != "0"
        true_positive = true_label != "0"

        if pred_positive and true_positive:
            TP += 1
        elif pred_positive and not true_positive:
            FP += 1
        elif not pred_positive and not true_positive:
            TN += 1
        elif not pred_positive and true_positive:
            FN += 1

        if pred == true_label:
            total_correct += 1

        per_line.append({
            "Line Number": idx,
            "Code Snippet": gt_entry["code_snippet"],
            "Ground Truth Label": true_label,
            "Predicted Label": pred,
            "Is Correct": pred == true_label
        })

    accuracy = total_correct / len(ground_truth)

    return {
        "Per-Line Metrics": per_line,
        "Accuracy": accuracy,
        "TP": TP,
        "FP": FP,
        "TN": TN,
        "FN": FN
    }


def save_td_per_line_metrics(results, exp_name, results_dir=RESULT_DIR):
    os.makedirs(results_dir, exist_ok=True)
    filename = os.path.join(results_dir, f"{exp_name}_per_line_metrics.csv")
    df = pd.DataFrame(results["Per-Line Metrics"])
    df.to_csv(filename, index=False)
    print(f"TD per-line metrics saved to: {filename}")


def save_td_summary_metrics(results, exp_name, results_dir=RESULT_DIR):
    os.makedirs(results_dir, exist_ok=True)
    summary_file = os.path.join(results_dir, f"{exp_name}_summary_metrics.csv")
    summary_df = pd.DataFrame([{
        "Accuracy": results["Accuracy"],
        "TP": results["TP"],
        "FP": results["FP"],
        "TN": results["TN"],
        "FN": results["FN"]
    }])
    summary_df.to_csv(summary_file, index=False)
    print(f"TD summary metrics saved to: {summary_file}")


def evaluate_and_save_td(normalize_fn, ground_truth, raw_preds, exp_name, results_dir=RESULT_DIR):
    """
    Wrapper function to normalize TD predictions, compute metrics, and save CSV files.

    Args:
        normalize_fn: Function to normalize raw predictions (e.g., convert "No smell"/"Blob" to "0"-"4")
        ground_truth: List of ground truth entries (from get_td_ground_truth)
        raw_preds: List of raw predictions generated by agents
        exp_name: Experiment/design name for file naming
        results_dir: Directory to save results
    """
    # Apply normalization to raw predictions
    normalized_preds = [normalize_fn(p) for p in raw_preds]

    # Compute per-line metrics and summary
    results = evaluate_td_per_line(ground_truth, normalized_preds)
    save_td_per_line_metrics(results, exp_name, results_dir)
    save_td_summary_metrics(results, exp_name, results_dir)

    # --- summary printout ---
    print("\n=== Technical Debt Detection Evaluation Summary ===")
    print(f"Accuracy: {results['Accuracy']:.2%}")
    print(f"TP: {results['TP']} | FP: {results['FP']} | TN: {results['TN']} | FN: {results['FN']}")
    print("====================================================\n")

    # Return only the summary info (not per-line details)
    return {
        "Accuracy": results["Accuracy"],
        "TP": results["TP"],
        "FP": results["FP"],
        "TN": results["TN"],
        "FN": results["FN"]
    }
